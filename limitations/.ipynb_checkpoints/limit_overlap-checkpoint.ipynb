{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.gmtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from _code import DeepLogModel\n",
    "from _code import generator\n",
    "from _code import trie\n",
    "from _code import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 15\n",
    "vocabulary = generator.make_vocabulary(vocabulary_size=vocabulary_size)\n",
    "patterns = generator.generate_patterns(num_patterns=100, vocabulary=vocabulary, min_pattern_size=3, max_pattern_size=7)\n",
    "p_add = [(\"a\",\"b\",\"c\",\"d\",\"e\"), (\"c\",\"d\",\"e\",\"f\",\"g\")]\n",
    "vocabulary_add = set(list(itertools.chain.from_iterable(p_add)))\n",
    "vocabulary = set(vocabulary)\n",
    "vocabulary = vocabulary.union(vocabulary_add)\n",
    "vocabulary_size = len(vocabulary)\n",
    "patterns = patterns + p_add\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, marks_train = generator.generate_text(patterns, text_size=10000, anomaly_ratio=0.00, vocabulary=vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_log_model = DeepLogModel.DeepLogModel(h=4, n=vocabulary_size, vocabulary=vocabulary)\n",
    "deep_log_model.build(num_lstm_layers=2, lstm_size=50) # for this example we don't want over-fit the network\n",
    "deep_log_model.fit(text_train,epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README\n",
    "1. We train the network on large corpus of workflow without over-fitting it.\n",
    "   (If the network is in over-fit state it has different problems. In that case our LSTM will be just an n-gram model with n=history_size)\n",
    "2. Two or the workflows in the training text are (a,b,c,d,e) and (c,d,e,f,g)\n",
    "3. We inject the anomalous text (a,b,c,d,e,f,g) inside a legitimate context\n",
    "4. An LSTM prediction which accept top-g predicted letters fail to alert on this anomaly.\n",
    "5. This is because an LSTM give a pretty high probability to letter f because its closest context (c,d,e) predict it, even so we have (a,b) before them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inject = (\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\")\n",
    "marks_inject = [1]*len((\"a\",\"b\",\"c\",\"d\",\"e\")) + [0]*len((\"f\",\"g\"))\n",
    "\n",
    "text_test_anomaly = patterns[1] + text_inject + patterns[2]\n",
    "print(text_test_anomaly)\n",
    "marks_test_anomaly = [1]*len(patterns[1]) + marks_inject + [1]*len(patterns[2])\n",
    "\n",
    "tests_legit = generator.generate_tests(patterns, vocabulary, n=1, text_size = 300, anomaly_ratio=0.00)\n",
    "text_test_legit, marks_test_legit, anomaly = tests_legit[0]\n",
    "\n",
    "entries = list()\n",
    "for g in range(0, vocabulary_size+1):\n",
    "    res_legit = deep_log_model.monitor_session(text_test_legit, marks_test_legit,g=g)\n",
    "    res_anomaly = deep_log_model.monitor_session(text_test_anomaly, marks_test_anomaly,g=g)\n",
    "    entry = (g,res_legit, res_anomaly)\n",
    "    entries.append(entry)\n",
    "df = pd.DataFrame(entries, columns = [\"g\", \"without_anomaly\", \"with_anomaly\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"total\"] = df[[\"without_anomaly\", \"with_anomaly\"]].min(axis=1)\n",
    "def color(x):\n",
    "    if x in {\"FP\",\"FN\"}:\n",
    "        return \"background-color: red\"\n",
    "    elif x in {\"TP\",\"TN\"}:\n",
    "        return \"background-color: green\"\n",
    "    else:\n",
    "        raise ValueError()\n",
    "        \n",
    "df.style.applymap(color, subset = [\"total\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
